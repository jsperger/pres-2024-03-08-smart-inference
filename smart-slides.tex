%%%%%
%%% Slides removed from presentation to refocus on value function inference
%
\begin{frame}[label={overview:time}]{Inference for DTRs}

	Differences in temporal setup: Single-stage, Multi-stage, Infinite Horizon
	\vfill
	Assumptions about these settings, e.g. recent seminar speaker Chengchun
	Shi's work on testing the Markov Assumption in MDPs
	\vfill
	Estimands: model parameters, value-function-related, and SMARTs have
	multiple non-DTR
	\vfill \pause
	Today's focus: Value Function Inference in the Single-stage Setting
\end{frame}


% \begin{frame}{SMARTs and DTRs are not synonymous}
% \end{frame}


\begin{frame}{General Notation \& Potential Outcomes}
	\vfill
	For a process $Z$ define $\overline{Z}_{\obsindex} = (Z_1, \ldots, Z_{\obsindex})$
	for a positive integer $\obsindex$.
	\pause \vfill
	Denote the potential outcome under a treatment sequence $\overline{\armobs} =
		(\armobs_1, \armobs_2)$ for an individual with covariates $\overline{\covarobs}$  by
	$$\po(\overline{\armobs}, \overline{\covarobs}) = \po(\covarrv_1 = \covarobs_1, \armrv_1 = \armobs_1, \covarrv_2 = \covarobs_2,\armrv_{2} = \armobs_{2})$$
	\pause \vfill
	We will suppress the dependence of $\po$ on $\overline{\covarobs}$ and write
	$$\po(\overline{\armobs}) = \po(\armrv_1 = \armobs_1, \armrv_{2} = \armobs_{2})$$
\end{frame}



\begin{frame}{Key Estimands}
	\textbf{Non-decision-making Estimands}
	\begin{itemize}
		\item Model Parameters
	\end{itemize}
	\vfill \pause

	\begin{itemize}
		\item Value of two embedded DTRs
	\end{itemize}

	\textbf{Treatment Policies}
	\begin{itemize}
		\item Identification of the best embedded treatment policy/eDTR
		\item Optimal policy $\optpol$
		\item Optimal policy $\optpol_{\symcal{F}}$ in a restrcited function
		      class $\symcal{F}$ (e.g. optimal policy among linear decision rules, trees of depth
		      $c$, or embedded policies)
	\end{itemize}
	\vfill \pause

	\textbf{Value of a Policy and Value Comparisons}
	\begin{itemize}
		\item Fixed: the value of a fixed policy or policies (usually the eDTRs) $\val(\pol)$
		\item Estimated: Value of the \textbf{estimated} optimal policy $\val(\polhat)$
		\item Optimal: Value of the optimal policy $\val(\optpol)$
		\item Comparison of non-overlapping policies e.g. most intensive vs. least intensive
	\end{itemize}

	\vfill \pause

	\footnotesize The difference between policy and value estimation is analagous to the
	difference between estimating the parameters of a linear model and estimating
	the average treatment effect.
\end{frame}


\section{OWL Hoot Hoot}
\begin{frame}{Assumptions}
\end{frame}

\begin{frame}{Method-specific Inference}
\end{frame}


\begin{frame}{Outcome Weighted Learning}
	Setup: Two treatments $\{-1, +1\} \subseteq \armspace$

	$$\E[\poa{1}(\covarobs) - \poa{-1}(\covarobs) \mid \covarrv = \covarobs]$$

\end{frame}

\begin{frame}{}
\end{frame}


\subsection{Loss Functions and their surrogates}
\begin{frame}{The $0$-$1$ loss}
\end{frame}

\begin{frame}{Hinge Loss}
\end{frame}
\subsection{Asymptotics --- What are we doing here?}
\begin{frame}{Why (when) do we trust asymptotic approximations?}
	Assuming standard regularity condition hold, what does the Central Limit
	Theorem tell us about $\Pn \covarrv$?

	Taylor Series
\end{frame}

\begin{frame}{Berry Essen Theorem}
	Suppose $\covarrv_1, \ldots, \covarrv_{\asymindex}$ are iid with
	\begin{align*}
		\E[X]     & = 0                 \\
		\E[X^2]   & = \sigma^2 < \infty \\
		\E[|X|^3] & = \rho < \infty
	\end{align*}
	And let $Y_{\asymindex} = \Pn \covarrv$. Then
	\vfill
	\begin{equation}
		\sup_{x \in \mathbb{R}}|F_{\asymindex}(\covarobs) - \Phi(\covarobs)| \leq \frac{C
			\rho}{\sigma^{3} \rootn }
	\end{equation}

	Where $C$ is a universal constant and according to the wiki I haven't fact
	checked our current best bounds are $.4097 \leq C \leq .4748$
	\vfill
\end{frame}

\begin{frame}{But wait --- aren't all treatment effects nonzero?}
	Even if the effect is $\epsilon^{-35}$ for some $\epsilon \ll 1$ we have asymptotic
	normality. Don't all treatments have an  effect at that level even if it's
	just the sugar in the pill?
\end{frame}



%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
